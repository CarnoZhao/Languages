{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "using MLDatasets, LinearAlgebra, Statistics, Images, Random\n",
    "\n",
    "train_x, train_y = MNIST.traindata(Float64)\n",
    "test_x, test_y = MNIST.testdata(Float64)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu(x) = max(0, x)\n",
    "sigmoid(x) = 1 / (1 + exp(-x))\n",
    "relu_derivative(y) = ifelse(y > 0, 1, 0)\n",
    "sigmoid_derivative(y) = y * (1 - y)\n",
    "\n",
    "function random_initialization(layer_dims)\n",
    "    parameters = []\n",
    "    L = length(layer_dims) - 1\n",
    "    for i in 1:L\n",
    "        W = randn(layer_dims[i + 1], layer_dims[i]) * sqrt(2 / layer_dims[i])\n",
    "        b = zeros(layer_dims[i + 1])\n",
    "        push!(parameters, [W, b])\n",
    "    end\n",
    "    parameters\n",
    "end\n",
    "\n",
    "function moment_initialization(parameters)\n",
    "    V = []\n",
    "    for (W, b) in parameters\n",
    "        vdW = zeros(size(W))\n",
    "        vdb = zeros(size(b))\n",
    "        push!(V, [vdW, vdb])\n",
    "    end\n",
    "    S = deepcopy(V)\n",
    "    V, S\n",
    "end\n",
    "\n",
    "function batch_data(X, Y, batch_size)\n",
    "    m = size(X)[2]\n",
    "    shuffle_X = X[:, shuffle(1:m)]\n",
    "    shuffle_Y = Y[:, shuffle(1:m)]\n",
    "    batches = []\n",
    "    k = Int(floor(m // batch_size))\n",
    "    for i in 1:k\n",
    "        mini_X = shuffle_X[:, ((k - 1) * batch_size + 1):ifelse(i == k, m, (k * batch_size))]\n",
    "        mini_Y = shuffle_Y[:, ((k - 1) * batch_size + 1):ifelse(i == k, m, (k * batch_size))]\n",
    "        push!(batches, [mini_X, mini_Y])\n",
    "    end\n",
    "    batches\n",
    "end\n",
    "\n",
    "function forward(X, parameters)\n",
    "    caches = [X]\n",
    "    A = X\n",
    "    for (i, (W, b)) in enumerate(parameters)\n",
    "        Z = W * A .+ b\n",
    "        A = ifelse(i == length(parameters), sigmoid.(Z), relu.(Z))\n",
    "        push!(caches, A)\n",
    "    end\n",
    "    caches\n",
    "end\n",
    "\n",
    "function backward(Y, parameters, caches, V, S, beta1, beta2)\n",
    "    L = length(parameters)\n",
    "    m = size(Y)[2]\n",
    "    AL = caches[length(caches)]\n",
    "    dA = (AL - Y) ./ (AL .* (1 .- AL))\n",
    "    for i in L:-1:1\n",
    "        A, A_prev, W = caches[i + 1], caches[i], parameters[i][1]\n",
    "        dZ = dA .* ifelse(i == L, sigmoid_derivative.(A), relu_derivative.(A))\n",
    "        dW = dZ * A_prev' / m\n",
    "        db = mean(dZ, dims = 2)\n",
    "        dA = W' * dZ\n",
    "        V[i][1] = beta1 * V[i][1] + (1 - beta1) * dW\n",
    "        V[i][2] = beta1 * V[i][2] + (1 - beta1) * db\n",
    "        S[i][1] = beta2 * S[i][1] + (1 - beta2) * dW .^ 2\n",
    "        S[i][2] = beta2 * S[i][2] + (1 - beta2) * db .^ 2\n",
    "    end\n",
    "    V, S\n",
    "end\n",
    "\n",
    "function update_parameters(V, S, epsilon, parameters, learning_rate)\n",
    "    L = length(parameters)\n",
    "    for i in 1:L\n",
    "        parameters[i][1] -= learning_rate * V[i][1] ./ (sqrt.(S[i][1]) .+ epsilon)\n",
    "        parameters[i][2] -= learning_rate * V[i][2] ./ (sqrt.(S[i][2]) .+ epsilon)\n",
    "    end\n",
    "    parameters\n",
    "end\n",
    "\n",
    "function compute_cost(AL, Y)\n",
    "    cost = -mean(Y .* log.(AL) + (1 .- Y) .* log.(1 .- AL))\n",
    "    cost\n",
    "end\n",
    "\n",
    "function predict(X, parameters)\n",
    "    caches = forward(X, parameters)\n",
    "    AL = caches[length(caches)]\n",
    "    num = argmax(AL)\n",
    "    num\n",
    "end\n",
    "\n",
    "function network(X, Y, layer_dims; num_iterations = 3000, batch_size = 64, learning_rate = 0.0075, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)\n",
    "    parameters = random_initialization(layer_dims)\n",
    "    V, S = moment_initialization(parameters)\n",
    "    for i in 1:num_iterations\n",
    "        cost = 0\n",
    "        batches = batch_data(X, Y, batch_size)\n",
    "        for (mini_X, mini_Y) in batches\n",
    "            caches = forward(mini_X, parameters)\n",
    "            cost += compute_cost(caches[length(caches)], mini_Y)\n",
    "            V, S = backward(mini_Y, parameters, caches, V, S, beta1, beta2)\n",
    "            parameters = update_parameters(V, S, epsilon, parameters, learning_rate)\n",
    "        end\n",
    "        if i % 100 == 0\n",
    "            println(\"Cost after $(i) iterations: $(round(cost, digits = 4))\")\n",
    "        end\n",
    "    end\n",
    "    parameters\n",
    "end\n",
    "\n",
    "function main(train_x, train_y, test_x, test_y; train_size = 6000, test_size = 100, num_iterations = 3000, batch_size = 64, learning_rate = 0.0075, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)\n",
    "    train_x = reshape(train_x, :, size(train_x)[length(size(train_x))])[:, 1:train_size]\n",
    "    train_y = [ifelse(i == y, 1, 0) for y in train_y for i in 0:9]\n",
    "    train_y = reshape(train_y, 10, :)[:, 1:train_size]\n",
    "    test_x = reshape(test_x, :, size(test_x)[length(size(test_x))])[:, 1:test_size]\n",
    "    test_y = [ifelse(i == y, 1, 0) for y in test_y for i in 0:9]\n",
    "    test_y = reshape(test_y, 10, :)[:, 1:test_size]\n",
    "    layer_dims = [size(train_x)[1], 20, 15, size(train_y)[1]]\n",
    "    parameters = network(train_x, train_y, layer_dims)\n",
    "    pred_train_y = predict(train_x, parameters)\n",
    "    pred_test_y = predict(test_x, parameters)\n",
    "    println(\"Train accuracy: $(round(mean(pred_train_y .== train_y) * 100, digits = 3))%\")\n",
    "    println(\"Test accuracy: $(round(mean(pred_test_y .== test_y) * 100, digits = 3))%\")\n",
    "    parameters\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 100 iterations: 29.1433\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] materialize at ./boot.jl:404 [inlined]",
      " [2] broadcast(::typeof(+), ::Array{Float64,2}, ::Array{Float64,2}) at ./broadcast.jl:707",
      " [3] +(::Array{Float64,2}, ::Array{Float64,2}) at ./arraymath.jl:47",
      " [4] backward(::Array{Float64,2}, ::Array{Any,1}, ::Array{Array{Float64,2},1}, ::Array{Any,1}, ::Array{Any,1}, ::Float64, ::Float64) at ./In[48]:66",
      " [5] #network#213(::Int64, ::Int64, ::Float64, ::Float64, ::Float64, ::Float64, ::Function, ::Array{Float64,2}, ::Array{Int64,2}, ::Array{Int64,1}) at ./In[48]:102",
      " [6] network at ./In[48]:94 [inlined]",
      " [7] #main#214(::Int64, ::Int64, ::Int64, ::Int64, ::Float64, ::Float64, ::Float64, ::Float64, ::typeof(main), ::Array{Float64,3}, ::Array{Int64,1}, ::Array{Float64,3}, ::Array{Int64,1}) at ./In[48]:120",
      " [8] main(::Array{Float64,3}, ::Array{Int64,1}, ::Array{Float64,3}, ::Array{Int64,1}) at ./In[48]:113",
      " [9] top-level scope at In[49]:1"
     ]
    }
   ],
   "source": [
    "parameters = main(train_x, train_y, test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
